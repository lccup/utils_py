"""
# df

用于操作pandas.DataFrame

> function
    show
    reindex_with_unique_col
    apply_merge_field

>> Path
    iter_dir
    walk_dir

>> gourp
    gourp_agg
    group_transform
    group_yield

>> sort
    sort_key_with_str_list

>> random
    random_choice
    random_choice_by_group

"""

import sys

from utils_py.general import Path, np, pd
from utils_py.general import handle_type_to_list
from IPython.display import display


def show(df, n=2):
    display(df.head(n), df.shape)


def reindex_with_unique_col(df, key, drop=False):
    assert df[key].is_unique, "[Error][not unique] {} column".format(key)
    df.index = df[key].to_numpy()
    if drop:
        df = df.drop(columns=[key])
    return df


def apply_merge_field(df, str_format, check_unique=False):
    res = df.apply(lambda row: str_format.format(**row), axis=1)
    if check_unique:
        assert res.is_unique, "[Error][not unique] {}".format(str_format)
    return res


def to_dict(df, key_key, key_value, check_key_unique=False):
    if check_key_unique:
        assert df[key_key].is_unique, "[Error] col '{}' is not unique".format(key_key)
    res = {k: v for k, v in zip(df[key_key], df[key_value])}
    return res


def get(df, indexs=None, columns=None):
    if indexs is None and columns is None:
        return df
    if indexs is None:
        indexs = df.index
    else:
        temp_indexs = [i for i in indexs if not i in df.index]
        if len(temp_indexs) > 0:
            print("[Waring] index not in df : {}".format(",".join(temp_indexs)))
            indexs = [i for i in indexs if not i in temp_indexs]
        del temp_indexs
    if columns is None:
        columns = df.columns
    else:
        temp_columns = [i for i in columns if not i in df.columns]
        if len(temp_columns) > 0:
            print("[Waring] index not in df : {}".format(",".join(temp_columns)))
            columns = [i for i in columns if not i in temp_columns]
        del temp_columns

    return df.loc[indexs, columns]


def ravel(df, name_value="value", order="C", sep=";"):
    assert (
        df.index.is_unique and df.columns.is_unique
    ), "[Error] index or columns is not unique"
    return pd.DataFrame(
        {name_value: np.ravel(df.to_numpy(), order=order)},
        index=pd.MultiIndex.from_product([df.index, df.columns], names=["ind", "col"])
        .to_series()
        .str.join(sep)
        .values,
    )


def product_seq(seq2d, names=None):
    """
    笛卡尔积

    Parameters
    ----------
    seq2d : list | np.arr | arr-like
        [[1,2,3,4],['a','b','c']]
    """
    names = ["col_{}".format(i) for i in range(len(seq2d))] if names is None else names
    return (
        pd.MultiIndex.from_product(seq2d, names=names).to_frame().reset_index(drop=True)
    )


def create_empty_df(dict_col_type=None):
    """
    构造并返回一个空的DataFrame, 需提供dict_col_type指定各列的类型
    """
    assert isinstance(dict_col_type, dict)
    return pd.DataFrame(columns=dict_col_type.keys()).astype(dict_col_type)


def ones(size, shape, index=None, columns=None, value=1):
    """
    构造并返回一个size个元素,形状为shape,由value填充的DataFrame
    """
    return pd.DataFrame(
        np.repeat(value, size).reshape(shape), index=index, columns=columns
    )


def ones_from_df(df, value=1):
    return ones(df.size, df.shape, df.index, df.columns, value)


##################################################
# path
##################################################


def iter_dir(p, path_match="", path_match_filter=[], select="f"):
    p = Path(p)
    assert p.exists(), "[not exists] {}".format(p)
    assert p.is_dir(), "[Error] p is not a dir"

    res = pd.DataFrame({"path": p.iterdir()})

    # select
    if select == "file" or select[0] == "f":
        res = res[res["path"].apply(lambda x: x.is_file())]
    elif select == "dir" or select[0] == "d":
        res = res[res["path"].apply(lambda x: x.is_dir())]
    else:
        # file and dir
        pass
    if res.shape[0] == 0:
        print("[Error] no item")
        return pd.DataFrame({"path": [], "name": []})

    if path_match:
        res = res[res["path"].apply(lambda x: x.match(path_match))]

    if path_match_filter and isinstance(path_match_filter, str):
        path_match_filter = [path_match_filter]
    for _ in path_match_filter:
        res = res[res["path"].apply(lambda x: not x.match(_))]

    res["name"] = res["path"].apply(lambda x: x.name)
    res = res.sort_values("name", ascending=True)
    res = res.reset_index(drop=True)
    return res.copy()


def walk_dir(p, select="f", drop_type=False):
    """
    深度遍历dir
    """

    def walk_with_os(p):
        import os

        files = np.array([])
        dirs = np.array([])
        for root, ds, fs in os.walk(p):
            if len(ds) > 0:
                dirs = np.concatenate([dirs, [os.path.join(root, d) for d in ds]])
            if len(fs) > 0:
                files = np.concatenate([files, [os.path.join(root, f) for f in fs]])
        return files, dirs

    def walk_with_Path(p):
        """
        Python 3.12以上才有 Path().walk
        sys.version_info.major >= 3  and  sys.version_info.minor >= 12
        """
        files = np.array([])
        dirs = np.array([])
        for root, ds, fs in p.walk():
            if len(ds) > 0:
                dirs = np.concatenate([dirs, [root.joinpath(d) for d in ds]])
            if len(fs) > 0:
                files = np.concatenate([files, [root.joinpath(f) for f in fs]])
        return files, dirs

    p = Path(p)
    assert p.exists(), "[not exists] {}".format(p)
    files, dirs = (
        walk_with_Path(p)
        if sys.version_info.major >= 3 and sys.version_info.minor >= 12
        else walk_with_os(p)
    )

    res = pd.concat(
        [
            pd.DataFrame({"path": files, "type": "f"}),
            pd.DataFrame({"path": dirs, "type": "d"}),
        ]
    )
    res["path"] = res["path"].apply(lambda x: Path(x))
    res["name"] = res["path"].apply(lambda x: x.name)
    res = res.loc[:, "path,name,type".split(",")]
    if select in "f,d".split(","):
        res = res.query("type == '{}' ".format(select))
        if drop_type:
            res = res.drop(columns="type")

    return res.reset_index(drop=True).copy()


##################################################
# group
##################################################


def group_agg(
    df,
    groupby_list,
    agg_dict=None,
    dropna=True,
    reindex=True,
    recolumn=True,
    rename_dict=None,
):
    groupby_list = handle_type_to_list(groupby_list)
    if None is agg_dict:
        agg_dict = {groupby_list[-1]: ["count"]}

    res = df.groupby(groupby_list, dropna=dropna, observed=False).agg(agg_dict)
    if recolumn:
        res.columns = ["_".join(i) for i in res.columns]
    if reindex:
        res = res.index.to_frame().join(res)
        res.index = np.arange(res.shape[0])
    if isinstance(rename_dict, dict):
        res = res.rename(columns=lambda k: rename_dict.setdefault(k, k))
    return res


def group_transform(df, groupby_list, key, func):
    """
    DataFrame.groupby.transform
    相比于agg, transform 会将各个元素对应的分组计算结果返回,其索引保持不变
    适合用于将分组统计的结果作为新的一列添加至df中

    Parameters
    ----------
    func : str | func
        count,sum,max,min,mean,median,std,var
        'sum' -> lambda x:x.sum()
    """

    return df.groupby(groupby_list)[key].transform(func)


def group_yield(
    df, key_group, keys_value, order=None, return_group=False, check_order=True
):
    """
    按组yield

    Parameters
    ----------
    order : str | list | None

    check_order : bool | default: True
        检查order 中的项 均存在于 df[key_group]
    """
    keys_value = handle_type_to_list(keys_value)
    order = df[key_group].unique() if order is None else order
    if check_order:
        assert (
            pd.Series(order).isin(df[key_group]).all()
        ), """[Error] not all ele of group in order
order special {}
group special {}
""".format(
            ",".join(np.setdiff1d(order, df[key_group])),
            ",".join(np.setdiff1d(df[key_group], order)),
        )

    if return_group:
        for g in order:
            yield g, df.loc[df[key_group] == g, keys_value]
    else:
        for g in order:
            yield df.loc[df[key_group] == g, keys_value]


##################################################
# sort
##################################################


def sort_key_with_str_list(str_list, ascending=True, return_order_dict=False):
    def str_list_to_order_dict(str_list, ascending=True):
        if isinstance(str_list, str):
            str_list = str_list.split(",")
        str_list = str_list if ascending else str_list[::-1]
        return {k: i for i, k in enumerate(str_list)}

    order_dict = str_list_to_order_dict(str_list, ascending=ascending)
    if return_order_dict:
        return order_dict

    return lambda s: s.map(lambda k: order_dict.setdefault(k, k))


##################################################
# Matrix
##################################################


def matrix_classify(df, key_index, key_column, func_agg="count", fill_na=0):
    """获取两个分类变量的混淆矩阵"""
    df = group_agg(df, [key_index, key_column], {key_column: [func_agg]})
    key_value = "__{}".format(func_agg) if isinstance(func_agg, str) else "__values"

    df.columns = [key_index, key_column, key_value]
    df = df.pivot(index=key_index, columns=key_column, values=key_value)
    if not fill_na is None:
        df = df.fillna(fill_na)
    return df


def matrix_numeric(df, groupby_list, key_value, func_agg, fill_na=0):
    """
    以groupby_list对df进行分组,使用func_agg进行聚合

    Parameters
    ----------
    groupby_list : list
        str list
        分组字段

    key_value : str
        数值字段

    func_agg : func | str
        min,max,mean,median,std,var
    """
    assert len(groupby_list) == 2
    df = df.groupby(groupby_list).agg({key_value: func_agg})
    key_value = "value_{}".format(key_value)
    df.columns = [key_value]
    df = df.reset_index().pivot(
        index=groupby_list[0], columns=groupby_list[1], values=key_value
    )
    if not fill_na is None:
        df = df.fillna(fill_na)
    return df


##################################################
# random
##################################################


def random_choice(arr, size, seed=None, replace=False):
    arr = np.array(arr)
    _rng = np.random.default_rng(seed)  # 若为None,则相当于没有设置种子
    return _rng.choice(arr, size, replace=replace)


def random_choice_by_group(df, key, percentage, seed=None, replace=False):
    """
    按组抽样 , 返回抽样后的index
    要求df.index是唯一的
    """
    assert df.index.is_unique
    assert key in df.columns

    df = df.loc[:, [key]].copy()
    df[key] = df[key].astype(str)
    df_stat = df[key].value_counts().to_frame("__count").reset_index(key)
    df_stat["__count_choice"] = np.ceil(df_stat["__count"] * percentage)
    df_stat["__count_choice"] = df_stat["__count_choice"].astype(int)
    if df_stat.query("__count < __count_choice").shape[0] > 0:
        print("[Waring][random_choice_by_group] __count < __count_choice")
        display(df_stat.query("__count <= __count_choice"))
    return np.concatenate(
        df_stat.apply(
            lambda row: random_choice(
                df[df[key] == row[key]].index,
                row["__count_choice"],
                seed=seed,
                replace=replace,
            ),
            axis=1,
        )
    )
